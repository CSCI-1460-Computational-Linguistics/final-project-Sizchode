{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b0MLqDYLdLHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4a9668-0575-45fe-fb3e-ffc91620fa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"drive/MyDrive/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hXiL6mlFmssL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b045de-2e7c-4df8-a732-fd21ff3a3209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\n",
        "response = requests.get('http://dong.li/lang2logic/seq2seq_jobqueries.zip')\n",
        "if response.status_code == 200:\n",
        "  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
        "  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(FILEPATH)\n",
        "  print(\"Extraction completed.\")\n",
        "else:\n",
        "  print(\"Failed to download the zip file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}\n",
        "\n",
        "# print(list(query_word2idx.items())[:10]) # Now Quey stands for  input\n",
        "# print(list(query_idx2word.items())[:10])\n",
        "\n",
        "# print(list(lf_idx2word.items())[:10]) # Now LF stands for  output\n",
        "# print(list(lf_idx2word.items())[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "  \"\"\"Defines a Dataset object for the Jobs dataset to be used with Dataloader\"\"\"\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list[tuple[list[int], list[int]]]\n",
        "      a list of outputs of __getitem__\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a batched set of input sequences and a batched set of target sequences\n",
        "  \"\"\"\n",
        "  src, tgt = default_collate(batch)\n",
        "  return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# See the dataloader Now!"
      ],
      "metadata": {
        "id": "d7FKFoRI9I_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 加载数据集\n",
        "jobs_train, jobs_test = build_datasets()\n",
        "\n",
        "# 2. 构建 DataLoader\n",
        "train_dataloader, test_dataloader = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "\n",
        "# 3. 打印 DataLoader 的内容\n",
        "print(\"Printing a few samples from the train DataLoader:\")\n",
        "for i, (queries, logical_forms) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"Queries Shape (Input): {queries.shape}\")\n",
        "    print(f\"Logical Forms Shape (Target): {logical_forms.shape}\")\n",
        "    break  # 只打印第一个 batch\n",
        "\n",
        "# 遍历 DataLoader 的前几个 batch\n",
        "for i, (queries, logical_forms) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"Queries: {queries}\")\n",
        "    print(f\"Logical Forms: {logical_forms}\")\n",
        "\n",
        "    break\n",
        "\n",
        "for i, (queries, logical_forms) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"Queries (Input): {queries}\")\n",
        "    print(f\"Logical Forms (Target): {logical_forms}\")\n",
        "    break  # 打印一个 batch 即可\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z10j4I5cNODQ",
        "outputId": "81ade914-394b-48c2-e13f-52bf05a7d921"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few samples from the train DataLoader:\n",
            "Batch 1:\n",
            "Queries Shape (Input): torch.Size([20, 20])\n",
            "Logical Forms Shape (Target): torch.Size([64, 20])\n",
            "Batch 1:\n",
            "Queries: tensor([[ 45,  13,  99,  13,  43,  13,  27,  37,  84,  20, 102,  13,  25,  21,\n",
            "          21,  39,  84,  75,   7,  67],\n",
            "        [ 21,  33,  50,  33,  59,  14,  41,  34,  89,   2,  11,  33,   4,  16,\n",
            "          16,  59,  89,  33,  59,  18],\n",
            "        [ 33,  14,  92,  14,  25,   9,  42,  24,  18,   9,  12,  14,  84,   2,\n",
            "           2,   7,  18,  14, 116,  59],\n",
            "        [ 27,  15,  76,  97,  14,  24,  40,  38, 100,   3,  21,  50,  85,  20,\n",
            "          20,   8, 100,  50,  63,   9],\n",
            "        [  2,  16,  63,  35,  35,   2,   9,  39, 101,   5, 103,  51,  59,   9,\n",
            "           9,   9, 101,  51,  21,  17],\n",
            "        [ 20,   9,   7,   7,   9,  20,   3,  40,  67,   6,  51,  35,  13,   3,\n",
            "           3,  24,  67,  92,  12,   5],\n",
            "        [  9,   3,  14,   8,  17, 125, 125, 125,  33, 125,   2,  11,  33, 125,\n",
            "         125,   2,  33,   7,  52,   6],\n",
            "        [  3, 125,   9,   9,  20, 125, 125, 125,  59, 125,  20,   8,  14, 125,\n",
            "         125,  20,  19,   8,  31, 125],\n",
            "        [125, 125,  61,  24,   3, 125, 125, 125,  75, 125,   9,  15,  35, 125,\n",
            "         125, 125,  63,   9,  18, 125],\n",
            "        [125, 125, 125,   2, 125, 125, 125, 125,  13, 125,   3,  16,   2, 125,\n",
            "         125, 125,   7,   3,   5, 125],\n",
            "        [125, 125, 125,  20, 125, 125, 125, 125,  33, 125, 125,   9,  20, 125,\n",
            "         125, 125,  14, 125,  74, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125,  14, 125, 125,   5,  15, 125,\n",
            "         125, 125,   9, 125,   9, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125,   9, 125, 125,   6,  16, 125,\n",
            "         125, 125,   3, 125,  61, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125,  15, 125, 125, 125,   9, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125,   5, 125, 125, 125,   3, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125,   6, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125]])\n",
            "Logical Forms: tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 2,  2,  2,  ...,  2,  2,  2],\n",
            "        ...,\n",
            "        [51, 51, 51,  ..., 51, 51, 51],\n",
            "        [51, 51, 51,  ..., 51, 51, 51],\n",
            "        [51, 51, 51,  ..., 51, 51, 51]])\n",
            "Batch 1:\n",
            "Queries (Input): tensor([[ 15,  25,  21,  79,  84,  18,  13,  25,  11,   7,  39,  45,  11,  25,\n",
            "          18,  76,  18,  15,   7,  67],\n",
            "        [ 16,   4,  16,  80,  89,  53,  14,  84,   8,  59,  64,  21,  12,  54,\n",
            "          19,  50,  19,  50,   8,  18],\n",
            "        [  2,  84,   9,  81,  18,  67,   2,  85, 124, 116,  23,  33,   2,  14,\n",
            "          34,  92,  35,  92,   9,  59],\n",
            "        [ 20,  85,  17,  48, 100,  17,  20,  13,  35,  63,  20,  27,  20,   9,\n",
            "          48,   7,  15, 124,  33,   9],\n",
            "        [  9,  14,   6,   7, 114,  35,   9,  33,   9,  21,   9,   9,   9,   5,\n",
            "           9,   4,  16,  16,  27,  17],\n",
            "        [ 34,  34, 125,   8,  19,   7,   3,   7,  33,  12,  34,  17,   3,   6,\n",
            "           3,  78,   2,  23,  25,   5],\n",
            "        [  3, 109, 125,   9,  61,  16, 125,  14,  72,  15,   3,   5, 125, 125,\n",
            "         125,  14,  20,  20,  14,   6],\n",
            "        [125,  17, 125,  61,  34,   9, 125,   9,  70,  16, 125,  22, 125, 125,\n",
            "         125,   9,   9,  34,  17, 125],\n",
            "        [125,  59, 125, 125,   7,  24, 125,  17, 125,  52, 125, 125, 125, 125,\n",
            "         125,   3,  21,  33,  10, 125],\n",
            "        [125,   9, 125, 125,  27,   2, 125,  10, 125,  31, 125, 125, 125, 125,\n",
            "         125, 125,   3,  48, 125, 125],\n",
            "        [125,  17, 125, 125,   2,  20, 125, 125, 125,  18, 125, 125, 125, 125,\n",
            "         125, 125, 125,   9, 125, 125],\n",
            "        [125,   6, 125, 125,  20, 125, 125, 125, 125,   5, 125, 125, 125, 125,\n",
            "         125, 125, 125,   3, 125, 125],\n",
            "        [125, 125, 125, 125,   9, 125, 125, 125, 125,  22, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125,   3, 125, 125, 125, 125,   9, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125,   3, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125],\n",
            "        [125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125,\n",
            "         125, 125, 125, 125, 125, 125]])\n",
            "Logical Forms (Target): tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [21,  1,  1,  ...,  1, 20,  1],\n",
            "        [ 2,  2,  2,  ...,  2,  2,  2],\n",
            "        ...,\n",
            "        [51, 51, 51,  ..., 51, 51, 51],\n",
            "        [51, 51, 51,  ..., 51, 51, 51],\n",
            "        [51, 51, 51,  ..., 51, 51, 51]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import: Import all packages that I might used here!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abU-qJ8DRRPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import random\n",
        "from torch import optim\n",
        "from timeit import default_timer as timer\n",
        "from torch.optim import RMSprop\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "eXps0dfPRYit"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# TODO: Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NG376y1VUkOh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "QUERY_VOCAB_LEN = len(query_vocab)  # Input vocabulary size\n",
        "LF_VOCAB_LEN = len(lf_vocab)  # Output vocabulary size\n",
        "emb_dim = 128  # Embedding dimension\n",
        "hid_dim = 256  # Hidden state dimension for LSTM\n",
        "n_layers = 2  # Number of LSTM layers\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim: int = 256, output_dim: int = LF_VOCAB_LEN):\n",
        "        super().__init__()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.w0 = nn.Linear(hid_dim, output_dim)  # Final output\n",
        "        self.w1 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w2 = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    def forward(self, hidden: torch.FloatTensor, enc_outputs: torch.FloatTensor, dec_outputs: torch.FloatTensor):\n",
        "        \"\"\"\n",
        "        Attention mechanism to compute context vector.\n",
        "        \"\"\"\n",
        "        return self.w0(self.tanh(\n",
        "            self.w1(dec_outputs)\n",
        "             + self.w2(\n",
        "                 torch.sum(self.softmax(enc_outputs * hidden[-1])\n",
        "                 * enc_outputs, dim=0) )))\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Seq2Seq model combining Encoder, Decoder, and Attention modules.\n",
        "    \"\"\"\n",
        "\n",
        "    class Encoder(nn.Module):\n",
        "        def __init__(self, input_dim: int, emb_dim: int, hid_dim: int, n_layers: int):\n",
        "            super().__init__()\n",
        "            # Define a two layer lstm, fetch hidden using hidden[0], hidden[1], cell[0],cell[1]\n",
        "            self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers)\n",
        "            # According to rubric, use nn.Embedding\n",
        "            self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=emb_dim)\n",
        "\n",
        "        def forward(self, enc_input: torch.Tensor) -> dict:\n",
        "            embedding = self.embedding(enc_input)\n",
        "            # Use a state_dict to store returned info and manage data\n",
        "            outputs, (hidden, cell) = self.lstm(embedding)\n",
        "            return {\"outputs\": outputs, \"hidden\": hidden, \"cell\": cell}\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        def __init__(self, output_dim: int, emb_dim: int, hid_dim: int, n_layers: int):\n",
        "            super().__init__()\n",
        "            # Define a two layer lstm, fetch hidden using hidden[0], hidden[1], cell[0],cell[1]\n",
        "            self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers)\n",
        "            # According to rubric, use nn.Embedding\n",
        "            self.embedding = nn.Embedding(num_embeddings=output_dim, embedding_dim=emb_dim)\n",
        "\n",
        "        def forward(self, dec_input: torch.Tensor, state_dict: dict) -> dict:\n",
        "            embedding = self.embedding(dec_input)\n",
        "            # Read state_dict passed from encoder, in this case, we use hidden output to initialize decoder!\n",
        "            outputs, (hidden, cell) = self.lstm(embedding, (state_dict[\"hidden\"], state_dict[\"cell\"]))\n",
        "            state_dict[\"outputs\"] = outputs\n",
        "            state_dict[\"hidden\"] = hidden\n",
        "            state_dict[\"cell\"] = cell\n",
        "            return state_dict\n",
        "\n",
        "    def __init__(self, device: str, input_dim: int, output_dim: int, emb_dim: int, hid_dim: int, n_layers: int):\n",
        "        super().__init__()\n",
        "        self.encoder = self.Encoder(input_dim, emb_dim, hid_dim, n_layers).to(device)\n",
        "        self.decoder = self.Decoder(output_dim, emb_dim, hid_dim, n_layers).to(device)\n",
        "        self.attention = Attention(hid_dim=hid_dim, output_dim=output_dim).to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Loss function and optimizer\n",
        "        self.criterion = nn.NLLLoss(ignore_index=lf_word2idx[\"<PAD>\"])\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.001)\n",
        "\n",
        "    def get_loss(self):\n",
        "        return self.criterion\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return self.optimizer\n",
        "\n",
        "    def forward(self, input_batch: torch.Tensor, query_batch: torch.Tensor, teacher_forcing_ratio: float = 1.0) -> torch.Tensor:\n",
        "        query_size, batch_size = query_batch.shape\n",
        "        query_vocab_size = self.decoder.embedding.num_embeddings\n",
        "        outputs = torch.zeros(query_size, batch_size, query_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        state_dict_enc = self.encoder(input_batch)\n",
        "        enc_outputs, hidden, cell = state_dict_enc[\"outputs\"], state_dict_enc[\"hidden\"], state_dict_enc[\"cell\"]\n",
        "\n",
        "        # Start decoding with <SOS> token\n",
        "        query = query_batch[0].unsqueeze(0)\n",
        "\n",
        "        for i in range(1, query_size):\n",
        "            state_dict_dec = self.decoder(query, state_dict_enc) # in this case, we use hidden output for encoder to initialize decoder!\n",
        "            outputs[i] = self.attention(state_dict_dec[\"hidden\"], enc_outputs, state_dict_dec[\"outputs\"])\n",
        "            best_pred = torch.argmax(outputs[i], dim=-1)\n",
        "\n",
        "            if teacher_forcing_ratio == 1.0: # Training and Use ground truth\n",
        "                query = query_batch[i].unsqueeze(0)\n",
        "            else: # Testing and  Use model's prediction\n",
        "                query = best_pred.unsqueeze(0)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Model creation function\n",
        "def create_model():\n",
        "    return Seq2Seq(\n",
        "        device=device,\n",
        "        input_dim=QUERY_VOCAB_LEN,\n",
        "        output_dim=LF_VOCAB_LEN,\n",
        "        emb_dim=emb_dim,\n",
        "        hid_dim=hid_dim,\n",
        "        n_layers=n_layers,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# TODO: Training and testing loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(LF_SOS_INDEX, LF_EOS_INDEX, LF_PAD_INDEX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVGJ88JB3oFS",
        "outputId": "cf932542-d698-4867-815c-ccb81577aad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 5 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UT5eiZM0AnTf"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int = 5,\n",
        "          device: str = \"cuda\") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Trains a given model on the provided DataLoader using RMSProp and NLLLoss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        The Seq2Seq model to train.\n",
        "    train_dataloader : DataLoader\n",
        "        DataLoader containing the training dataset.\n",
        "    num_epochs : int\n",
        "        Number of epochs to train the model.\n",
        "    device : str\n",
        "        Device to run the training on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    nn.Module\n",
        "        The trained model.\n",
        "    \"\"\"\n",
        "    # Define the loss function\n",
        "    criterion = model.get_loss()\n",
        "    optimizer = model.get_optimizer()\n",
        "\n",
        "    # Add a scheduler for dynamic learning rate adjustment\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Halve LR every 2 epochs\n",
        "    # Move the model to the appropriate device\n",
        "    model = model.to(device)\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "        epoch_loss = 0\n",
        "        start_time = timer()\n",
        "\n",
        "        for queries, logical_forms in train_dataloader:\n",
        "            # Move data to device\n",
        "            queries = queries.to(device)\n",
        "            logical_forms = logical_forms.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(queries, logical_forms, 1.0)\n",
        "\n",
        "            # Compute log probabilities for NLLLoss\n",
        "            log_probs = outputs.log_softmax(dim=-1)\n",
        "\n",
        "            # Reshape outputs and targets for loss computation\n",
        "            log_probs = log_probs.view(-1, log_probs.size(-1))  # [seq_len * batch_size, output_dim]\n",
        "            targets = logical_forms.view(-1)                   # [seq_len * batch_size]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(log_probs, targets)\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for the epoch\n",
        "            epoch_loss += loss.item()\n",
        "        end_time = timer()\n",
        "\n",
        "        # Print epoch loss\n",
        "        train_loss = epoch_loss / len(train_dataloader)\n",
        "        print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str = \"cuda\") -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Evaluates the model on the provided DataLoader using greedy decoding.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        The trained Seq2Seq model.\n",
        "    dataloader : DataLoader\n",
        "        DataLoader containing evaluation data.\n",
        "    device : str\n",
        "        Device to run the evaluation on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple\n",
        "        Per-token accuracy and exact-match accuracy.\n",
        "    \"\"\"\n",
        "    # Initialize metrics as a dictionary\n",
        "    metrics = {\n",
        "        \"per_token_count\": 0,\n",
        "        \"per_token_total\": 0,\n",
        "        \"exact_match_count\": 0,\n",
        "        \"total_sequences\": 0,\n",
        "    }\n",
        "    teacher_forcing = 0.0  # Disabled for evaluation\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for input, query in dataloader:\n",
        "            preds = []\n",
        "            truth = []\n",
        "            exact_match_flag = True  # For exact-match accuracy\n",
        "\n",
        "            # Move data to the device\n",
        "            input, query = input.to(device), query.to(device)\n",
        "\n",
        "            # Forward pass with teacher forcing disabled\n",
        "            logits = model(input, query, teacher_forcing_ratio=teacher_forcing)\n",
        "            pred = torch.argmax(logits, dim=-1)  # Get predictions\n",
        "            flattened_pred = pred.view(-1)\n",
        "            flattened_query = query.view(-1)\n",
        "            # for p, q in zip(pred[1:].squeeze(), query[1:].squeeze()):\n",
        "            for p, q in zip(flattened_pred, flattened_query):\n",
        "                if q.item() != LF_EOS_INDEX and p != q:\n",
        "                    exact_match_flag = False  # Mark sequence as incorrect if one token mismatches\n",
        "\n",
        "                if q.item() == LF_EOS_INDEX:  # Stop when <EOS> is encountered\n",
        "                    break\n",
        "\n",
        "                if p == q:  # Token matches\n",
        "                    metrics[\"per_token_count\"] += 1\n",
        "\n",
        "                metrics[\"per_token_total\"] += 1\n",
        "\n",
        "                preds.append(query_idx2word[p.item()])\n",
        "                truth.append(query_idx2word[q.item()])\n",
        "\n",
        "            # Update exact match count if the sequence matches entirely\n",
        "            if exact_match_flag:\n",
        "                metrics[\"exact_match_count\"] += 1\n",
        "\n",
        "            # Increment total sequences\n",
        "            metrics[\"total_sequences\"] += 1\n",
        "\n",
        "    # Compute accuracies\n",
        "    per_token_acc = metrics[\"per_token_count\"] / metrics[\"per_token_total\"] if metrics[\"per_token_total\"] > 0 else 0.0\n",
        "    exact_match_acc = metrics[\"exact_match_count\"] / metrics[\"total_sequences\"] if metrics[\"total_sequences\"] > 0 else 0.0\n",
        "\n",
        "    print(f\"Per-token Accuracy: {per_token_acc:.4f}\")\n",
        "    print(f\"Exact-match Accuracy: {exact_match_acc:.4f}\")\n",
        "\n",
        "    return per_token_acc, exact_match_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Define specific seeds to use\n",
        "    # seeds = [52]\n",
        "    seeds = [52,62, 72]\n",
        "    per_token_accuracies = []\n",
        "    exact_match_accuracies = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"Running for seed: {seed}\")\n",
        "        # Set seed for reproducibility\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "        # Initialize datasets and dataloaders\n",
        "        jobs_train, jobs_test = build_datasets()\n",
        "        dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = create_model()\n",
        "        model = train(model, dataloader_train, num_epochs=20, device=device)\n",
        "\n",
        "        # Evaluate the model\n",
        "        test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "\n",
        "        # Store the results\n",
        "        per_token_accuracies.append(test_per_token_accuracy)\n",
        "        exact_match_accuracies.append(test_exact_match_accuracy)\n",
        "\n",
        "        print(f\"Seed {seed}: Test Per-token Accuracy = {test_per_token_accuracy:.4f}, \"\n",
        "              f\"Test Exact-match Accuracy = {test_exact_match_accuracy:.4f}\")\n",
        "\n",
        "    # Compute averages\n",
        "    avg_per_token_accuracy = sum(per_token_accuracies) / len(seeds)\n",
        "    avg_exact_match_accuracy = sum(exact_match_accuracies) / len(seeds)\n",
        "\n",
        "    # Print average results\n",
        "    print(f\"Average Per-token Accuracy over seeds {seeds}: {avg_per_token_accuracy:.4f}\")\n",
        "    print(f\"Average Exact-match Accuracy over seeds {seeds}: {avg_exact_match_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UfG8jPw8SYv",
        "outputId": "79a6fb3b-e99b-4efb-bd24-bcf270b8cf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for seed: 52\n",
            "Epoch: 1, Train loss: 2.007, Epoch time = 7.353s\n",
            "Epoch: 2, Train loss: 0.916, Epoch time = 3.473s\n",
            "Epoch: 3, Train loss: 0.803, Epoch time = 3.719s\n",
            "Epoch: 4, Train loss: 0.697, Epoch time = 3.772s\n",
            "Epoch: 5, Train loss: 0.615, Epoch time = 4.078s\n",
            "Epoch: 6, Train loss: 0.550, Epoch time = 3.356s\n",
            "Epoch: 7, Train loss: 0.508, Epoch time = 3.349s\n",
            "Epoch: 8, Train loss: 0.467, Epoch time = 5.174s\n",
            "Epoch: 9, Train loss: 0.427, Epoch time = 3.396s\n",
            "Epoch: 10, Train loss: 0.398, Epoch time = 3.355s\n",
            "Epoch: 11, Train loss: 0.377, Epoch time = 3.769s\n",
            "Epoch: 12, Train loss: 0.350, Epoch time = 4.174s\n",
            "Epoch: 13, Train loss: 0.320, Epoch time = 3.401s\n",
            "Epoch: 14, Train loss: 0.315, Epoch time = 3.372s\n",
            "Epoch: 15, Train loss: 0.283, Epoch time = 4.415s\n",
            "Epoch: 16, Train loss: 0.269, Epoch time = 3.378s\n",
            "Epoch: 17, Train loss: 0.266, Epoch time = 3.334s\n",
            "Epoch: 18, Train loss: 0.282, Epoch time = 3.528s\n",
            "Epoch: 19, Train loss: 0.247, Epoch time = 4.231s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qSnLCPeiI1N"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     jobs_train, jobs_test = build_datasets()\n",
        "#     dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "#     model = create_model()\n",
        "#     model = train(model, dataloader_train, num_epochs=5, device=device)\n",
        "#     test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "#     print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "#     print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "# main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "9mewu8d2qACH",
        "_hfJFfYRSFBV",
        "RCKjb4HsMKw-",
        "d7FKFoRI9I_c",
        "abU-qJ8DRRPI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}